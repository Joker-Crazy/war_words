{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_year_month(date_time_str):\n",
    "    return date_time_str[:7]\n",
    "\n",
    "def normalize(arr):\n",
    "    return (arr - np.mean(arr)) / np.std(arr) / np.sqrt(len(arr))\n",
    "\n",
    "def group_monthly(kremlin_data):\n",
    "    tmp = kremlin_data.loc['datetime'].apply(lambda x: x[:7]).value_counts().to_dict()\n",
    "    dt_min = dt.strptime(min(kremlin_data.loc['datetime'])[:7], '%Y-%m')\n",
    "    dt_max = dt.strptime(max(kremlin_data.loc['datetime'])[:7], '%Y-%m')\n",
    "    dt_list = [dt.strftime(dt_min+relativedelta(months=+x), \"%Y-%m\") for x in range((dt_max.year-dt_min.year)*12+(dt_max.month-dt_min.month)+1)]\n",
    "    monthly_kremlin = {item:{'count': tmp.get(item, 0), 'header': '', 'summary': '', 'content': ''} for item in dt_list}\n",
    "#     for item in tmp:\n",
    "#         monthly_kremlin[item] = {'count': tmp[item], 'header': '', 'summary': '', 'content': ''}\n",
    "    for item in kremlin_data:\n",
    "        monthly_kremlin[get_year_month(kremlin_data[item]['datetime'])]['header'] += ' ' + kremlin_data[item]['header']\n",
    "        monthly_kremlin[get_year_month(kremlin_data[item]['datetime'])]['summary'] += ' ' + kremlin_data[item]['summary']\n",
    "        monthly_kremlin[get_year_month(kremlin_data[item]['datetime'])]['content'] += ' ' + kremlin_data[item]['content']\n",
    "    \n",
    "    return pd.DataFrame.from_dict(monthly_kremlin)\n",
    "    \n",
    "def calculate_freq_per_month(phrase_dict):\n",
    "    words = [val for sublist in phrase_dict.values() for val in sublist]\n",
    "    monthly_words_decomposition = {}\n",
    "    for key in phrase_dict:\n",
    "        tmp = dict(zip(set(words), [0] * len(set(words))))\n",
    "        for word in phrase_dict[key]:\n",
    "            tmp[word] += 1\n",
    "        monthly_words_decomposition[key] = tmp\n",
    "    return pd.DataFrame.from_dict(monthly_words_decomposition)\n",
    "\n",
    "def plot_word_correlation(arr_a, arr_b, arr_corr, word, save_name):\n",
    "    x = np.arange(0., len(arr_b))/12\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, ncols=1, constrained_layout=True,figsize=(9, 9),dpi=150)\n",
    "    \n",
    "    a = np.zeros_like(x)\n",
    "    a[-len(arr_a):] = arr_a\n",
    "    b = np.zeros_like(x)\n",
    "    b[:len(arr_b)] = arr_b\n",
    "    c = np.zeros_like(x)\n",
    "    c[:len(arr_corr)] = arr_corr\n",
    "    \n",
    "    ax0.plot(x, a, 'r')\n",
    "    ax0.set_title('Статистика бойових втрат')\n",
    "    ax0.set_xticks(np.arange(0, max(x), 1))\n",
    "    ax0.grid()\n",
    "\n",
    "    ax1.plot(x, b)\n",
    "    ax1.set_title('Корінь \"'+ word + '\"')\n",
    "    ax1.set_xticks(np.arange(0, max(x), 1))\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(x, c)\n",
    "    ax2.set_title('кореляційна функція')\n",
    "    ax2.set_xticks(np.arange(0, max(x), 1))\n",
    "    ax2.grid()\n",
    "\n",
    "    fig.savefig(save_name)\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "def NormData(arr = np.array(0)):\n",
    "    arrSD = np.std(arr)\n",
    "    arrAM = np.mean(arr)\n",
    "    tmp = (arr - arrAM) / arrSD\n",
    "    return tmp\n",
    "\n",
    "def PearsonCorr(Base = np.array(0), part = np.array(0)):\n",
    "    if len(Base) == 0:\n",
    "        return 'Invalid input data'\n",
    "    else:\n",
    "        tm = np.float128(0)\n",
    "        ptmp = NormData(part)\n",
    "        Nn = len(part)\n",
    "        res = np.zeros(len(Base)-Nn)\n",
    "        for ii in range(len(Base)-Nn):\n",
    "            tmp0 = NormData(Base[ii:ii + Nn])\n",
    "            for jj in range(Nn):\n",
    "                tm = tm + ptmp[jj]*tmp0[jj]\n",
    "            res[ii] = tm / (Nn-1)\n",
    "            tm = 0.0\n",
    "        return res\n",
    "\n",
    "def fftCorr(Base = np.array(0), part = np.array(0)):\n",
    "    part0 = np.zeros(len(Base))\n",
    "    normPart = NormData(part)\n",
    "    part0[:len(normPart)] = normPart[:]\n",
    "    normBase = NormData(Base)\n",
    "    nBsp = np.fft.fft(normBase)\n",
    "    nP0sp = np.fft.fft(part0)\n",
    "    nBsp = nBsp * np.conj(nP0sp)\n",
    "    res = np.real(np.fft.ifft(nBsp))/len(part)\n",
    "    return res[:-len(part)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defenders_losses = pd.read_json('results/killed_defenders.json').applymap(get_year_month)\n",
    "defenders_losses['https://uk.wikipedia.org'].value_counts().sort_index().to_csv('defenders.csv')\n",
    "defenders_losses['https://uk.wikipedia.org'].value_counts().sort_index().plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kremlin_data = pd.read_json('results/kremlin_data.json')\n",
    "# kremlin_data.loc['datetime'].apply(lambda x: x[:7]).value_counts().to_csv('kremlin_freg.csv')\n",
    "kremlin_data.loc['datetime'].apply(get_year_month).value_counts().sort_index().plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_kremlin = group_monthly(kremlin_data)\n",
    "monthly_kremlin.loc['count'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# biwords\n",
    "additional_punctuation = '«»'\n",
    "all_punct = string.punctuation + additional_punctuation\n",
    "monthly_header = monthly_kremlin.loc['content'].apply(\n",
    "    lambda w: list(ngrams(nltk.word_tokenize(w.lower()), 1))\n",
    ").to_dict()\n",
    "monthly_header_stemmed = {}\n",
    "ps = SnowballStemmer('russian')\n",
    "for key in monthly_header:\n",
    "    tmp = []\n",
    "    for item in monthly_header[key]:\n",
    "        if not any(val in all_punct for val in item):\n",
    "            tmp.append(tuple([ps.stem(word) for word in item]))\n",
    "    monthly_header_stemmed[key] = tmp\n",
    "header_monthly_words_decomposition = pd.DataFrame.sort_index(\n",
    "    calculate_freq_per_month(monthly_header_stemmed),\n",
    "    axis=1\n",
    ")\n",
    "header_monthly_words_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = {}\n",
    "defend = defenders_losses['https://uk.wikipedia.org'].value_counts().sort_index().to_numpy()\n",
    "for word in header_monthly_words_decomposition.T:\n",
    "    words = header_monthly_words_decomposition.loc[word].sort_index().to_numpy()\n",
    "#     if sum(words) < 30:\n",
    "#         continue\n",
    "    corr_tmp = fftCorr(\n",
    "        Base=normalize(words),\n",
    "        part=normalize(defend)\n",
    "    )\n",
    "    count_words[' '.join(word)] = {'sum': sum(words), 'max': corr_tmp.max(), 'min': corr_tmp.min()}\n",
    "#     sv_name = 'results/corr-{:0>3}_count-{:0>6}_root-{}.png'.format(\n",
    "#         np.round(int(corr_tmp.max()*100)),\n",
    "#         sum(words),\n",
    "#         '_'.join(word)\n",
    "#     )\n",
    "    \n",
    "#     plot_word_correlation(defend, words, corr_tmp, '_'.join(word), sv_name)\n",
    "count_words = pd.DataFrame.from_dict(count_words).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words.to_csv('results/01_words_corr_content_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
